{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e99d7e",
   "metadata": {},
   "source": [
    "## RETRIEVE ADDITIONAL DATA FROM KNOW YOUR MEMES WEBSITE: \n",
    "\n",
    "- AUTHOR, \n",
    "- LAST UPDATER, \n",
    "- ADDED DATE, \n",
    "- LAST UPDATE DATE, \n",
    "- NUMBER OF VIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adf451fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # pip install beautifulsoup4\n",
    "import nltk\n",
    "import re\n",
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "path_to_cleaned = \"cleaned_data//\"\n",
    "fname_main_df = 'main_df.csv'\n",
    "df_main = pd.read_csv(path_to_cleaned + fname_main_df, sep=';', index_col=0, encoding='utf-8') \n",
    "\n",
    "# Web crawling rules: https://knowyourmeme.com/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "837b4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_email = ' ' # for bot header  \n",
    "result_file = \"web_scraping.txt\" # results will be gathered here\n",
    "\n",
    "start_idx = 11964\n",
    "end_idx = 12196 # 12196  # len(ids) # 8491/12196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332e7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(i, ids, headers):\n",
    "    url = ids[i]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    status = response.status_code\n",
    "    if status==200:\n",
    "        content = response.text\n",
    "    else: \n",
    "        content = None    \n",
    "        \n",
    "    return [status, content]\n",
    "\n",
    "# Status codes:  200 - ok, 401- no access rights, 404- not found, 500- server broke "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a3409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(content):\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Views: \n",
    "    c =  soup.find_all(\"div\", class_ = \"views\")[0] \n",
    "    c.find_all(\"div\", \"title\")\n",
    "    c.find_all(re.compile('title'))\n",
    "    views = c.get('title')  # '136,011 Views'\n",
    "    views = views.split(' ')[0] \n",
    "    views = re.sub(',', '', views) # 136011\n",
    "       \n",
    "    # Added_by, updated_by, add_date, update_date: \n",
    "    b = soup.find_all(\"div\", class_ = \"author-info\")[0]\n",
    "    p_list = b.find_all(\"p\")\n",
    "    added_by, updated_by, added_date, update_date = \"\", \"\", \"\", \"\"\n",
    "    \n",
    "    for p in p_list:\n",
    "        if \"Added\" in p.get_text().strip():\n",
    "            a = p.find_all(\"a\")[0]\n",
    "            added_by = a.attrs[\"href\"].split('users/')[-1]\n",
    "            t = p.find_all(\"abbr\")[0]\n",
    "            added_date = t.attrs[\"title\"].split('T')[0]\n",
    "            \n",
    "        elif \"Updated\" in p.get_text().strip():\n",
    "            a = p.find_all(\"a\")[0]\n",
    "            updated_by = a.attrs[\"href\"].split('users/')[-1]\n",
    "            t = p.find_all(\"abbr\")[0]\n",
    "            update_date = t.attrs[\"title\"].split('T')[0]\n",
    "    \n",
    "    return [views, added_by, updated_by, added_date, update_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078ebe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bot: \n",
    "bot_header = \"univ. course project\"  # \"dataeng (\" + your_email + \") / student project\"\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update(\n",
    "  { 'User-Agent': bot_header, }\n",
    ")\n",
    "\n",
    "# Links:\n",
    "#id_df = pd.read_excel('id_df.xlsx', index_col=0)  \n",
    "#ids = id_df['url'].tolist()\n",
    "df_main['url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6084b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping:\n",
    "for i in range(start_idx, end_idx):  # len(ids) # 12196\n",
    "    status, content = retrieve_data(i, ids, headers)\n",
    "    if status!=200: # some error occurred\n",
    "        result = [str(i), ids[i], str(status), \"0\", \"\", \"\", \"\", \"\"] # to record bad links\n",
    "    else:\n",
    "        result = [str(i), ids[i], str(status)] + parse_html(content)\n",
    "        \n",
    "    row = \",\".join(result) # for writing list to txt file\n",
    "        \n",
    "    # write to file:\n",
    "    with open(result_file, \"a\", encoding=\"UTF-8\") as f:\n",
    "        f.write(row)\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Report everything is going fine after 50 requests (ca 8.3 min):\n",
    "    if i%50==0:\n",
    "        print(i)   \n",
    "    time.sleep(30) # delay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
